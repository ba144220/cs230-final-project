{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "from typing import List, Dict, Any\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "import numpy as np\n",
    "\n",
    "from constants.prompts import SYSTEM_PROMPT, ASSISTANT_PREFIX\n",
    "from utils.datasets_loader import load_single_dataset\n",
    "from parsers.argument_classes import ModelArguments, DatasetArguments, TrainingArguments, PeftArguments\n",
    "\n",
    "\n",
    "\n",
    "class DataCollatorForGridTokenization():\n",
    "    def __init__(\n",
    "        self, \n",
    "        tokenizer: PreTrainedTokenizerFast,\n",
    "        max_seq_length: int,\n",
    "        is_train: bool = True,\n",
    "        is_grid_tokenization: bool = False,\n",
    "        system_prompt: str = SYSTEM_PROMPT,\n",
    "        assistant_prefix: str = ASSISTANT_PREFIX,\n",
    "        end_header_id: int = 128007,\n",
    "        line_length: int = 1000,\n",
    "        \n",
    "    ):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.is_train = is_train\n",
    "        self.end_header_id = end_header_id\n",
    "        self.system_prompt = system_prompt\n",
    "        self.assistant_prefix = assistant_prefix\n",
    "        self.is_grid_tokenization = is_grid_tokenization\n",
    "        self.line_length = line_length\n",
    "        self.table_pad_token = tokenizer.eos_token\n",
    "        self.start_of_line_token = \"[START_OF_LINE]\"\n",
    "        self.end_of_line_token = \"[END_OF_LINE]\"\n",
    "        self.table_cell_separator_token = \"[TABLE_CELL_SEPARATOR]\"\n",
    "        self.extension_separator_map = {\n",
    "            \"csv\": \",\",\n",
    "            \"html\": \" \",\n",
    "            \"tsv\": \"\\t\"\n",
    "        }\n",
    "\n",
    "        if self.is_grid_tokenization:\n",
    "            new_tokens = [self.start_of_line_token, self.end_of_line_token, self.table_cell_separator_token]\n",
    "            self.tokenizer.add_tokens(new_tokens)\n",
    "        # Set tokenizer padding and padding side\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.tokenizer.padding_side = \"left\"\n",
    "        self.tokenizer.return_tensors = \"pt\"\n",
    "        self.tokenizer.max_length = 1024\n",
    "    \n",
    "    def _get_label(self, batch: Dict[str, torch.Tensor]):\n",
    "        \"\"\"\n",
    "        Get the label for the batch\n",
    "        Set all labels before the last occurrence of the end_header_id to -100\n",
    "        \"\"\"\n",
    "        if not self.is_train:\n",
    "            return batch\n",
    "        \n",
    "        batch[\"labels\"] = batch[\"input_ids\"].clone()\n",
    "        # Set all labels before the last occurrence of the end_header_id to -100\n",
    "        last_occurrence_indices = self._get_last_occurrence_indices(batch[\"input_ids\"], self.end_header_id)\n",
    "        for i in range(batch[\"input_ids\"].size(0)):\n",
    "            batch[\"labels\"][i, :last_occurrence_indices[i] + 2] = -100\n",
    "            \n",
    "        return batch\n",
    "    \n",
    "    \n",
    "    def _get_table_with_grid(self, context: str):\n",
    "        # TODO: check how to read the table correctly\n",
    "        # Remove .csv extension from the context if present\n",
    "        context = re.sub(r\"\\.csv$\", \"\", context)\n",
    "        separator = self.extension_separator_map[self.table_extension]\n",
    "        modified_lines = []\n",
    "\n",
    "        for line in f:\n",
    "            cells = line.strip().split(separator)\n",
    "            modified_line = self.start_of_line_token + self.table_cell_separator_token.join(cells) + self.end_of_line_token\n",
    "            modified_lines.append(modified_line.strip())\n",
    "        # Join the modified lines into a single string\n",
    "        return \"\\n\".join(modified_lines)\n",
    "\n",
    "\n",
    "    def _call_normal(self, examples: List[Dict[str, Any]]):\n",
    "        text_list = []\n",
    "        for example in examples:\n",
    "            user_prompt = str(example[\"table\"]) + \"\\n\" + str(example[\"question\"])\n",
    "            message = [\n",
    "                {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt},\n",
    "                \n",
    "            ]\n",
    "            if self.is_train:\n",
    "                message.append({\"role\": \"assistant\", \"content\": self.assistant_prefix + str(example[\"answer\"])})\n",
    "            if self.is_train:\n",
    "                message_string = self.tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=False)\n",
    "            else:\n",
    "                message_string = self.tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=True)\n",
    "            text_list.append(message_string)\n",
    "            \n",
    "        batch = self.tokenizer(\n",
    "            text_list, \n",
    "            return_tensors=\"pt\", \n",
    "            padding=True, \n",
    "            truncation=True, \n",
    "            max_length=self.max_seq_length, \n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "        \n",
    "        batch = self._get_label(batch)\n",
    "            \n",
    "        return batch\n",
    "    \n",
    "        \n",
    "    def _call_grid_tokenization(self, examples: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        examples: a list of dictionaries with the following keys: question, answer, context, id, task, direction, size, table\n",
    "        return a dictionary with the following keys: input_ids, attention_mask, labels\n",
    "        \"\"\"\n",
    "        # Retrieve the table content based on the context provided in the example\n",
    "        max_length = 0\n",
    "\n",
    "        for example in examples:\n",
    "            table = self._get_table_with_grid(example[\"table\"])\n",
    "            \n",
    "            # Construct the user prompt using the specified order of fields\n",
    "            user_prompt = str(table) + \"\\n\" + str(example[\"question\"])\n",
    "            # Create the system and user messages for the chat template\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ]\n",
    "\n",
    "            # Apply the chat template to create the input string\n",
    "            text = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "            input_ids = self._grid_it(text)\n",
    "            example[\"input_ids\"] = input_ids\n",
    "            if len(input_ids) > max_length:\n",
    "                max_length = len(input_ids)\n",
    "\n",
    "        # Batch the examples\n",
    "\n",
    "        for i in range(len(examples[\"input_ids\"])):\n",
    "            if len(examples[\"input_ids\"][i]) < max_length:\n",
    "                examples[\"input_ids\"][i] = [self.tokenizer.pad_token_id] * (max_length - len(examples[\"input_ids\"][i])) + examples[\"input_ids\"][i]\n",
    "\n",
    "        examples[\"attention_mask\"] = [[1] * max_length] * len(examples[\"input_ids\"])\n",
    "        return examples\n",
    "    \n",
    "    def _grid_it(self, text):\n",
    "        # Seperate the text into before_table, table, and after_table\n",
    "        table_pattern = r\"(\\[START_OF_LINE\\].*?\\[END_OF_LINE\\](?:\\n\\[START_OF_LINE\\].*?\\[END_OF_LINE\\])*)\"\n",
    "        parts = re.split(table_pattern, text, maxsplit=1)\n",
    "        before_table = parts[0].strip()\n",
    "        table = parts[1].strip() if len(parts) > 1 else \"\"\n",
    "        after_table = parts[2].strip() if len(parts) > 2 else \"\"\n",
    "\n",
    "        # Special token ids\n",
    "        start_of_line_token = self.tokenizer.encode(self.start_of_line_token, add_special_tokens=False)[0]\n",
    "        end_of_line_token = self.tokenizer.encode(self.end_of_line_token, add_special_tokens=False)[0]\n",
    "        table_cell_separator_token = self.tokenizer.encode(self.table_cell_separator_token, add_special_tokens=False)[0]\n",
    "        pad_token_id = self.tokenizer.encode(self.table_pad_token, add_special_tokens=False)[0]\n",
    "\n",
    "        # before_table\n",
    "        before_table_tokens = self.tokenizer.encode(before_table, add_special_tokens=False)\n",
    "        before_table_pad_count = self.line_length - len(before_table_tokens) % self.line_length\n",
    "        before_table_tokens.extend([pad_token_id] * before_table_pad_count)\n",
    "\n",
    "        # table\n",
    "        table_tokens = self.tokenizer.encode(table, add_special_tokens=False)\n",
    "        rows = table.strip().split(\"\\n\")\n",
    "        col_count = len(rows[0].split(self.table_cell_separator_token))\n",
    "        row_count = len(rows)\n",
    "        table_grid = np.full((row_count, self.line_length), pad_token_id, dtype=object)\n",
    "\n",
    "        # Get the max token number per column\n",
    "        token_num_per_cell = []\n",
    "        token_row = []\n",
    "        token_counter_in_row = 0\n",
    "        token_counter_in_cell = 0\n",
    "        for id in table_tokens:\n",
    "            token_counter_in_row += 1\n",
    "            \n",
    "            if id == start_of_line_token:\n",
    "                token_row = []\n",
    "                token_counter_in_cell = 0\n",
    "                token_counter_in_row = 1\n",
    "            elif id == end_of_line_token:\n",
    "                token_row.append(token_counter_in_cell)\n",
    "                token_num_per_cell.append(token_row)\n",
    "                token_row = []\n",
    "                token_counter_in_cell = 0\n",
    "            elif id == table_cell_separator_token:\n",
    "                token_row.append(token_counter_in_cell)\n",
    "                token_counter_in_cell = 0\n",
    "            else:\n",
    "                token_counter_in_cell += 1\n",
    "        token_num_per_cell = np.array(token_num_per_cell)\n",
    "        max_token_num_per_cell = np.max(token_num_per_cell, axis = 0)\n",
    "        pad_token_count = self.line_length - sum(max_token_num_per_cell)\n",
    "        if pad_token_count < 0:\n",
    "            print(\"The token number in the row exceeds the line length\")\n",
    "            # TODO: discard this data\n",
    "        \n",
    "\n",
    "        # Fill the table grid\n",
    "        token_col_cursor = self.line_length - 1\n",
    "        token_row_cursor = row_count - 1\n",
    "        cell_col_cursor = col_count - 1\n",
    "        cell_inner_token_counter = 0\n",
    "\n",
    "        for id in reversed(table_tokens):\n",
    "            current_cell_token_num = max_token_num_per_cell[cell_col_cursor]\n",
    "            if id == start_of_line_token:\n",
    "                token_row_cursor -= 1\n",
    "            elif id == end_of_line_token:\n",
    "                cell_col_cursor = col_count - 1\n",
    "                token_col_cursor = self.line_length - 1\n",
    "                table_grid[token_row_cursor, token_col_cursor - pad_token_count + 1:] = pad_token_id\n",
    "                token_col_cursor -= pad_token_count\n",
    "                cell_inner_token_counter = 0\n",
    "            elif id == table_cell_separator_token:\n",
    "                need_to_pad_token_count = current_cell_token_num - cell_inner_token_counter\n",
    "                table_grid[token_row_cursor, token_col_cursor - need_to_pad_token_count + 1 : token_col_cursor + 1] = pad_token_id\n",
    "                token_col_cursor -= need_to_pad_token_count\n",
    "                cell_inner_token_counter = 0\n",
    "            else:\n",
    "                table_grid[token_row_cursor, token_col_cursor] = id\n",
    "                token_col_cursor -= 1\n",
    "                cell_inner_token_counter += 1\n",
    "        \n",
    "        # after_table\n",
    "        after_table_tokens = self.tokenizer.encode(after_table, add_special_tokens=False)\n",
    "        after_table_pad_count = self.line_length - len(after_table_tokens) % self.line_length\n",
    "        after_table_tokens.extend([pad_token_id] * after_table_pad_count)\n",
    "\n",
    "        # Concatenate the three grids and flatten the result\n",
    "        result = np.concatenate((before_table_tokens, table_grid.flatten(), after_table_tokens), axis=0)\n",
    "        # for i in result:\n",
    "        #     print(i, self.tokenizer.decode(i))\n",
    "        # print(result)\n",
    "        return result\n",
    "\n",
    "        \n",
    "    def __call__(self, examples: List[Dict[str, Any]]):\n",
    "        \"\"\"\n",
    "        Columns: question, answer, context, id, task, direction, size, table\n",
    "        Only use question, table, and answer\n",
    "        \"\"\"\n",
    "        if self.is_grid_tokenization:\n",
    "            return self._call_grid_tokenization(examples)\n",
    "        else:\n",
    "            return self._call_normal(examples)\n",
    "        \n",
    "\n",
    "    def _get_last_occurrence_indices(self, input_ids, X):\n",
    "        \"\"\"\n",
    "        input_ids: 2D tensor of shape B x L\n",
    "        X: the integer value to find in the tensor\n",
    "        \"\"\"\n",
    "\n",
    "        # Create a boolean mask where elements equal to X are True\n",
    "        mask = (input_ids == X)  # Shape: B x L\n",
    "\n",
    "        # Reverse the mask along the sequence dimension (dimension 1)\n",
    "        reversed_mask = torch.flip(mask, dims=[1])  # Shape: B x L\n",
    "\n",
    "        # Find the index of the first occurrence of True in the reversed mask\n",
    "        # Convert boolean mask to float to use argmax (True becomes 1.0, False becomes 0.0)\n",
    "        idx_in_reversed = reversed_mask.float().argmax(dim=1)  # Shape: B\n",
    "\n",
    "        # Calculate the last occurrence index in the original tensor\n",
    "        last_indices = input_ids.size(1) - idx_in_reversed - 1  # Shape: B\n",
    "\n",
    "        # Handle rows where X does not occur\n",
    "        # If X does not occur in a row, the entire mask row is False, and argmax returns 0\n",
    "        # We need to set last_indices for these rows to -1 or any invalid index as per your requirements\n",
    "        has_X = mask.any(dim=1)  # Shape: B (True if X is in the row)\n",
    "        last_indices[~has_X] = -1  # Set to -1 where X does not occur\n",
    "\n",
    "        return last_indices.unsqueeze(1)  # Shape: B x 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_args = DatasetArguments(\n",
    "    table_extension = 'csv',\n",
    "    train_max_samples_for_each_dataset = 8,\n",
    "    val_max_samples_for_each_dataset = 2,\n",
    "    test_max_samples_for_each_dataset = 2,\n",
    "    shuffle_seed = 42\n",
    ")\n",
    "datasets = load_single_dataset(\"self_generated\", dataset_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
