# Model arguments
model_name: meta-llama/Llama-3.2-1B-Instruct
adapter_path: ./outputs/run_20241205_182848
load_in_4bit: false
load_in_8bit: true

# Table Llama arguments
line_length: 64
x_channels_start: 0
x_channels_end: 32
x_channels_step: 32
# y_channels_start: 0
# y_channels_end: 1
# y_channels_step: 1

# Dataset arguments
dataset_root_dir: ./datasets
dataset_names: ["wtq"]
table_extension: csv
train_max_samples_for_each_dataset: 80
val_max_samples_for_each_dataset: 80
test_max_samples_for_each_dataset: 960
shuffle_seed: 42

max_table_row_num: 40
max_table_width: 64

# Training arguments
batch_size: 8
max_seq_length: 3072

