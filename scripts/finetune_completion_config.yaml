# Model arguments
model_name: meta-llama/Llama-3.2-1B-Instruct
load_in_4bit: true
load_in_8bit: false

# Training arguments
gradient_accumulation_steps: 1
batch_size: 1
num_train_epochs: 1
save_total_limit: 3
logging_steps: 10
max_seq_length: 1024
dry_run: false

# Dataset arguments
dataset_root_dir: ./datasets
dataset_names: ["self_generated", "wtq"]
table_extension: csv
train_max_samples_for_each_dataset: 80
val_max_samples_for_each_dataset: 80
test_max_samples_for_each_dataset: 80
shuffle_seed: 42

# Peft arguments
r: 16
lora_alpha: 32
lora_dropout: 0.05
bias: none
task_type: CAUSAL_LM