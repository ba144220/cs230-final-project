{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.utils.checkpoint\n",
    "from torch import nn\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers.models.llama.configuration_llama import LlamaConfig\n",
    "from transformers.models.llama.modeling_llama import *\n",
    "\n",
    "MODEL_NAME = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "model = LlamaForCausalLM.from_pretrained(MODEL_NAME)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, padding_side=\"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Model and tokenizer loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-15): 16 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLlamaConfig(LlamaConfig):\n",
    "    def __init__(\n",
    "        self,\n",
    "        rate=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.rate = rate\n",
    "\n",
    "    # self.position_embeddings = position_embeddings\n",
    "    # Override the `to_dict` method to include the new parameters\n",
    "    def to_dict(self):\n",
    "        base_dict = super().to_dict()\n",
    "        config_dict = {\n",
    "            \"rate\": self.rate,\n",
    "        }\n",
    "        base_dict.update(config_dict)\n",
    "        return base_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_array(X, file_name):\n",
    "    \"\"\"\n",
    "    Saves array X to csv\n",
    "\n",
    "    Args:\n",
    "        X: numpy array of shape (d, seq_len) where d is the embedding dimension.\n",
    "    Returns:\n",
    "        void: saves X to csv.\n",
    "    \"\"\"\n",
    "    # df = pd.DataFrame(X)\n",
    "    # Convert the tensor to a NumPy array\n",
    "    numpy_array = X.numpy()\n",
    "    # Save the NumPy array to a file\n",
    "    file = './custom-llama/%s.npy' %(file_name)\n",
    "    np.save(file, numpy_array)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Copied from https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L85\n",
    "\"\"\"\n",
    "class CustomLlamaRotaryEmbedding(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim=None,\n",
    "        max_position_embeddings=2048,\n",
    "        base=10000,\n",
    "        device=None,\n",
    "        scaling_factor=1.0,\n",
    "        rope_type=\"default\",\n",
    "        config: Optional[CustomLlamaConfig] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # TODO (joao): remove the `if` below, only used for BC\n",
    "        self.rope_kwargs = {}\n",
    "        if config is None:\n",
    "            logger.warning_once(\n",
    "                \"`LlamaRotaryEmbedding` can now be fully parameterized by passing the model config through the \"\n",
    "                \"`config` argument. All other arguments will be removed in v4.45\"\n",
    "            )\n",
    "            self.rope_kwargs = {\n",
    "                \"rope_type\": rope_type,\n",
    "                \"factor\": scaling_factor,\n",
    "                \"dim\": dim,\n",
    "                \"base\": base,\n",
    "                \"max_position_embeddings\": max_position_embeddings,\n",
    "            }\n",
    "            self.rope_type = rope_type\n",
    "            self.max_seq_len_cached = max_position_embeddings\n",
    "            self.original_max_seq_len = max_position_embeddings\n",
    "        else:\n",
    "            # BC: \"rope_type\" was originally \"type\"\n",
    "            if config.rope_scaling is not None:\n",
    "                self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n",
    "            else:\n",
    "                self.rope_type = \"default\"\n",
    "            self.max_seq_len_cached = config.max_position_embeddings\n",
    "            self.original_max_seq_len = config.max_position_embeddings\n",
    "\n",
    "        self.config = config\n",
    "        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n",
    "        \n",
    "        # inverse frequency llama and attention factor\n",
    "        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "        self.original_inv_freq = self.inv_freq\n",
    "\n",
    "    def _dynamic_frequency_update(self, position_ids, device):\n",
    "        \"\"\"\n",
    "        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n",
    "        1 - growing beyond the cached sequence length (allow scaling)\n",
    "        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n",
    "        \"\"\"\n",
    "        seq_len = torch.max(position_ids) + 1\n",
    "        if seq_len > self.max_seq_len_cached:  # growth\n",
    "            inv_freq, self.attention_scaling = self.rope_init_fn(\n",
    "                self.config, device, seq_len=seq_len, **self.rope_kwargs\n",
    "            )\n",
    "            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n",
    "            self.max_seq_len_cached = seq_len\n",
    "\n",
    "        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n",
    "            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n",
    "            self.max_seq_len_cached = self.original_max_seq_len\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x, position_ids):\n",
    "        if \"dynamic\" in self.rope_type:\n",
    "            self._dynamic_frequency_update(position_ids, device=x.device)\n",
    "\n",
    "        # Core RoPE block\n",
    "        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n",
    "        position_ids_expanded = position_ids[:, None, :].float()\n",
    "        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n",
    "        device_type = x.device.type\n",
    "        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n",
    "        with torch.autocast(device_type=device_type, enabled=False):\n",
    "            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n",
    "            emb = torch.cat((freqs, freqs), dim=-1)\n",
    "            cos = emb.cos()\n",
    "            sin = emb.sin()\n",
    "        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n",
    "        cos = cos * self.attention_scaling\n",
    "        sin = sin * self.attention_scaling\n",
    "\n",
    "        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Copied from https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L524\n",
    "\"\"\"\n",
    "class CustomLlamaAttention(LlamaAttention):\n",
    "    def __init__(self, config: CustomLlamaConfig, layer_idx: Optional[int] = None):\n",
    "        super().__init__(config, layer_idx)\n",
    "        # Use custom rotary embedding\n",
    "        self.rotary_emb = CustomLlamaRotaryEmbedding(config=self.config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor, #pass\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_value: Optional[Cache] = None,\n",
    "        output_attentions: bool = False,\n",
    "        use_cache: bool = False,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.45 pass\n",
    "        **kwargs,\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "        if output_attentions:\n",
    "            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n",
    "            logger.warning_once(\n",
    "                \"LlamaModel is using LlamaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n",
    "                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n",
    "            )\n",
    "            return super().forward(\n",
    "                hidden_states=hidden_states,\n",
    "                attention_mask=attention_mask,\n",
    "                position_ids=position_ids,\n",
    "                past_key_value=past_key_value,\n",
    "                output_attentions=output_attentions,\n",
    "                use_cache=use_cache,\n",
    "                cache_position=cache_position,\n",
    "                position_embeddings=position_embeddings,\n",
    "            )\n",
    "\n",
    "        bsz, q_len, _ = hidden_states.size()\n",
    "\n",
    "        query_states = self.q_proj(hidden_states)\n",
    "        key_states = self.k_proj(hidden_states)\n",
    "        value_states = self.v_proj(hidden_states)\n",
    "\n",
    "        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "        if position_embeddings is None:\n",
    "            logger.warning_once(\n",
    "                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n",
    "                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n",
    "                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.45 `position_ids` will be \"\n",
    "                \"removed and `position_embeddings` will be mandatory.\"\n",
    "            )\n",
    "            cos, sin = self.rotary_emb(value_states, position_ids)\n",
    "        else:\n",
    "            cos, sin = position_embeddings\n",
    "        \"\"\"\n",
    "        [batch_size, seq_len, heads, head_dim]\n",
    "        query_states,  torch.Size([1, 32, 13, 64]) #Figure out multihead\n",
    "        key_states,  torch.Size([1, 8, 13, 64])\n",
    "\n",
    "        [batch_size, seq_len, head_dim]\n",
    "        cos torch.Size([1, 13, 64])\n",
    "        sin torch.Size([1, 13, 64])\n",
    "\n",
    "        For each head, we apply rotation.\n",
    "        \"\"\"\n",
    "\n",
    "        save_array(query_states, 'query_states')\n",
    "        save_array(key_states, 'key_states')\n",
    "        query_states, key_states = apply_custom_rotary_pos_emb(query_states, key_states, cos, sin) # print q, k, cos, sin\n",
    "        save_array(query_states, 'query_states_rope_applied')\n",
    "        save_array(key_states, 'key_states_rope_applied')\n",
    "        #get q, k and test\n",
    "\n",
    "        if past_key_value is not None:\n",
    "            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n",
    "            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n",
    "            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n",
    "\n",
    "        key_states = repeat_kv(key_states, self.num_key_value_groups)\n",
    "        value_states = repeat_kv(value_states, self.num_key_value_groups)\n",
    "\n",
    "        causal_mask = attention_mask\n",
    "        if attention_mask is not None:\n",
    "            causal_mask = causal_mask[:, :, :, : key_states.shape[-2]]\n",
    "\n",
    "        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n",
    "        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n",
    "        if query_states.device.type == \"cuda\" and causal_mask is not None:\n",
    "            query_states = query_states.contiguous()\n",
    "            key_states = key_states.contiguous()\n",
    "            value_states = value_states.contiguous()\n",
    "\n",
    "        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n",
    "        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n",
    "        is_causal = True if causal_mask is None and q_len > 1 else False\n",
    "\n",
    "        attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
    "            query_states,\n",
    "            key_states,\n",
    "            value_states,\n",
    "            attn_mask=causal_mask,\n",
    "            dropout_p=self.attention_dropout if self.training else 0.0,\n",
    "            is_causal=is_causal,\n",
    "        )\n",
    "\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "        attn_output = attn_output.view(bsz, q_len, -1)\n",
    "\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "\n",
    "        return attn_output, None, past_key_value\n",
    "    \n",
    "def custom_rotate_half(x):\n",
    "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "def apply_custom_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n",
    "    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n",
    "\n",
    "    Args:\n",
    "        q (`torch.Tensor`): The query tensor.\n",
    "        k (`torch.Tensor`): The key tensor.\n",
    "        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n",
    "        sin (`torch.Tensor`): The sine part of the rotary embedding.\n",
    "        position_ids (`torch.Tensor`, *optional*):\n",
    "            Deprecated and unused.\n",
    "        unsqueeze_dim (`int`, *optional*, defaults to 1):\n",
    "            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n",
    "            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n",
    "            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n",
    "            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n",
    "            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n",
    "            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n",
    "    Returns:\n",
    "        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n",
    "    \"\"\"\n",
    "    cos = cos.unsqueeze(unsqueeze_dim)\n",
    "    sin = sin.unsqueeze(unsqueeze_dim)\n",
    "    q_embed = (q * cos) + (custom_rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (custom_rotate_half(k) * sin)\n",
    "    return q_embed, k_embed\n",
    "\n",
    "\n",
    "class CustomLlamaDecoderLayer(LlamaDecoderLayer):\n",
    "\n",
    "    def __init__(self, config: CustomLlamaConfig, layer_idx: int):  # Add layer_idx as an argument\n",
    "        super().__init__(config, layer_idx)\n",
    "        self.self_attn = CustomLlamaAttention(config, layer_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLlamaModel(LlamaModel):\n",
    "\n",
    "    def __init__(self, config: CustomLlamaConfig):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [CustomLlamaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n",
    "        )\n",
    "        # Use custom rotary embedding\n",
    "        self.rotary_emb = CustomLlamaRotaryEmbedding(config=config)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "\"\"\"\n",
    "Top level model we are using\n",
    "\"\"\"\n",
    "class CustomLlamaForCausalLM(LlamaForCausalLM):\n",
    "    def __init__(self, config: CustomLlamaConfig):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.model = CustomLlamaModel(config)\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def save_checkpoint(self, dir):\n",
    "        # to bypass the code line 2291 in transformers.trainer\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_lamma_config = CustomLlamaConfig(rate=1, rope_type='default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_model = CustomLlamaForCausalLM.from_pretrained(MODEL_NAME)\n",
    "custom_model.config = custom_lamma_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Hey, are you conscious? Can you talk to me?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "generate_ids = custom_model.generate(inputs.input_ids, max_length=30)\n",
    "tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfering Weights to Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In scenarios where the custom model introduces changes to the layers, as in this example, \n",
    "the dimensions of the weights will differ. Then it is necessary to create new model weights tailored\n",
    "to the custom architecture.\n",
    "\"\"\"\n",
    "module_patterns_to_transfer = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "def transfer_weights(original_model, custom_model, module_patterns_to_transfer):\n",
    "    original_dict = original_model.state_dict()\n",
    "    custom_dict = custom_model.state_dict()\n",
    "\n",
    "    # Filter and transfer weights for specified layers\n",
    "    for key in custom_dict.keys():\n",
    "        for pattern in module_patterns_to_transfer:\n",
    "            if pattern in key:\n",
    "                if key in original_dict:\n",
    "                    # Transfer weights\n",
    "                    with torch.no_grad():\n",
    "                        custom_dict[key].copy_(original_dict[key])\n",
    "\n",
    "    # Load the updated state dictionary to the model\n",
    "    custom_model.load_state_dict(custom_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer weights from the original model to the model\n",
    "model_2 = CustomLlamaModel(custom_lamma_config)\n",
    "transfer_weights(model, model_2, module_patterns_to_transfer)\n",
    "\n",
    "# transferred weights in the custom model\n",
    "for key, parameter in model_2.state_dict().items():\n",
    "    print(key)\n",
    "    print(parameter.size())\n",
    "    print(parameter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the new weights into a folder.\n",
    "model_2.save_pretrained('./custom-llama-weights/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs230",
   "language": "python",
   "name": "cs230"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
