{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"../src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collators.data_collator_for_grid_tokenization import DataCollatorForGridTokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LINE_LENGTH = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = DataCollatorForGridTokenization(\n",
    "    tokenizer, \n",
    "    1024,\n",
    "    is_train=False,\n",
    "    is_grid_tokenization=True,\n",
    "    line_length=LINE_LENGTH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_text = \"Hello, world! This is a very long string that should be padded to the nearest multiple of 32. We will also include the end of text token at the end.\"\n",
    "input_text = \"\"\n",
    "token_ids = collator._grid_tokenize_string(\n",
    "    input_text,\n",
    "    include_eot=True,\n",
    "    include_header=True,\n",
    "    header_content=\"system\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrettyPrintTokens:\n",
    "    def __init__(self, len_for_each_token=4, line_length=32):\n",
    "        self.len_for_each_token = len_for_each_token\n",
    "        self.line_length = line_length\n",
    "    \n",
    "        \n",
    "    def __call__(self, tokens, len_for_each_token=None, line_length=None):\n",
    "        len_for_each_token = len_for_each_token or self.len_for_each_token\n",
    "        line_length = line_length or self.line_length\n",
    "        \n",
    "        tkns = [tkn.replace(\"Ä \", \"_\") for tkn in tokens ]\n",
    "        tkns = [str(i) for i in range(line_length)] + tkns\n",
    "        res = []\n",
    "        for tkn in tkns:\n",
    "            if len(tkn) <= len_for_each_token:\n",
    "                # pad the token with spaces\n",
    "                res.append(\" \" * (len_for_each_token - len(tkn)) + tkn)\n",
    "            else:\n",
    "                res.append(tkn[:len_for_each_token])\n",
    "        \n",
    "        # Group the tokens into lines of length line_length\n",
    "        for i in range(0, len(res), line_length):\n",
    "            row_idx = i // line_length\n",
    "            row_idx_str = str(row_idx).rjust(3, \" \")\n",
    "            print(row_idx_str + \": \" + \" \".join(res[i:i+line_length]))\n",
    "        \n",
    "pretty_printer = PrettyPrintTokens(\n",
    "    len_for_each_token=4,\n",
    "    line_length=LINE_LENGTH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(input_text)\n",
    "print(\"-\"*100)\n",
    "tkns = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "pretty_printer(tkns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = [\n",
    "    [\"\", \"Name\", \"Age\", \"Email\", \"Phone\"],\n",
    "    [\"Row 1\", \"John\", \"25\", \"john@example.com\", \"123-456-7890\"],\n",
    "    [\"Row 2\", \"Jane\", \"30\", \"jane@example.com\", \"098-765-4321\"],\n",
    "    [\"Row 3\", \"Christopher\", \"36\", \"chris.testing@verylongcompany.com\", \"123-123-1234\"],\n",
    "]\n",
    "token_ids = collator._grid_tokenize_table(table)\n",
    "tkns = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "print(\"-\"*100)\n",
    "pretty_printer(tkns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_string = \"\"\"\n",
    ",Name,Age,Email,Phone\n",
    "Row 1,John,25,john@example.com,123-456-7890\n",
    "Row 2,Jane,30,jane@example.com,098-765-4321\n",
    "Row 3,Christopher,36,chris.testing@verylongcompany.com,123-123-1234\n",
    "\"\"\"\n",
    "table = collator._convert_csv_string_to_table(csv_string)\n",
    "print(table)\n",
    "token_ids = collator._grid_tokenize_table(table)\n",
    "tkns = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "print(\"-\"*100)\n",
    "pretty_printer(tkns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parsers.argument_classes import DatasetArguments\n",
    "from utils.datasets_loader import load_datasets\n",
    "\n",
    "dataset_args = DatasetArguments(\n",
    "    dataset_root_dir=\"../datasets\",\n",
    "    dataset_names=[\"self_generated\"],\n",
    "    table_extension=\"csv\",\n",
    "    train_max_samples_for_each_dataset=100,\n",
    "    val_max_samples_for_each_dataset=100,\n",
    "    test_max_samples_for_each_dataset=100,\n",
    ")\n",
    "\n",
    "datasets = load_datasets(dataset_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "print(datasets[\"test\"][idx][\"table\"])\n",
    "print(\"-\"*100)\n",
    "print(datasets[\"test\"][idx][\"question\"])\n",
    "print(\"-\"*100)\n",
    "print(datasets[\"test\"][idx][\"answer\"])\n",
    "print(\"-\"*100)\n",
    "collator.is_train = False\n",
    "input_ids = collator._grid_tokenize_example(datasets[\"test\"][idx])\n",
    "tkns = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "pretty_printer(tkns, len_for_each_token=30, line_length=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "collator.is_train = True\n",
    "batch = collator([datasets[\"train\"][1], datasets[\"train\"][2], datasets[\"train\"][3]])\n",
    "print(batch)\n",
    "print(batch[\"input_ids\"][2])\n",
    "print(batch[\"attention_mask\"][2])\n",
    "if \"labels\" in batch:\n",
    "    print(batch[\"labels\"][2])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tkns = tokenizer.convert_ids_to_tokens(batch[\"input_ids\"][0])\n",
    "attn_mask = [str(i) for i in batch[\"attention_mask\"][0].tolist()]\n",
    "labels = [str(i) for i in batch[\"labels\"][1].tolist()]\n",
    "\n",
    "\n",
    "\n",
    "pretty_printer(tkns, len_for_each_token=5, line_length=64)\n",
    "\n",
    "pretty_printer(labels, len_for_each_token=5, line_length=64)\n",
    "pretty_printer(attn_mask, len_for_each_token=5, line_length=64)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens([128009])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[\"train\"][\"size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "collator.is_train = True\n",
    "collator.is_grid_tokenization = False\n",
    "batch = collator([datasets[\"train\"][1], datasets[\"train\"][2], datasets[\"train\"][3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch[\"labels\"][1])\n",
    "print(batch[\"attention_mask\"][1])\n",
    "print(batch[\"input_ids\"][1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stress test the collator\n",
    "collator = DataCollatorForGridTokenization(\n",
    "    tokenizer, \n",
    "    1024,\n",
    "    is_train=True,\n",
    "    is_grid_tokenization=True,\n",
    "    line_length=64,\n",
    ")\n",
    "\n",
    "\n",
    "dataset_args = DatasetArguments(\n",
    "    dataset_root_dir=\"../datasets\",\n",
    "    dataset_names=[\"wtq\"],\n",
    "    table_extension=\"csv\",\n",
    "    train_max_samples_for_each_dataset=100,\n",
    "    val_max_samples_for_each_dataset=100,\n",
    "    test_max_samples_for_each_dataset=100,\n",
    ")\n",
    "\n",
    "datasets = load_datasets(dataset_args)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "dataloader = DataLoader(datasets[\"train\"], batch_size=8, collate_fn=collator)\n",
    "for batch in dataloader:\n",
    "    print(batch[\"input_ids\"].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs230",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
