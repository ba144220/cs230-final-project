{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# Add ../src to the system path\n",
    "src_path = os.path.abspath(os.path.join(os.path.dirname(\"__file__\"), \"../src\"))\n",
    "datasets_path = os.path.abspath(os.path.join(os.path.dirname(\"__file__\"), \"../datasets\"))\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "if datasets_path not in sys.path:\n",
    "    sys.path.append(datasets_path)\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaConfig, BitsAndBytesConfig\n",
    "from transformers.models.llama.modeling_llama import LlamaRotaryEmbedding\n",
    "\n",
    "load_dotenv()\n",
    "# With grid\n",
    "from models.modeling_table_llama import TableLlamaConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"meta-llama/Llama-3.2-1B-Instruct\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing TableLlamaForCausalLM\n",
    "* With entire channel set to zero \n",
    "* With gaussian noise added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_llama_config = TableLlamaConfig.from_pretrained(MODEL_NAME)\n",
    "\n",
    "table_llama_config.rope_table_llama = {\n",
    "    \"x_channels_start\": None,\n",
    "    \"x_channels_end\": None,\n",
    "    \"x_channels_step\": None,\n",
    "    \"y_channels_start\": None,\n",
    "    \"y_channels_end\": None,\n",
    "    \"y_channels_step\": None,\n",
    "    \"line_length\": None\n",
    "}\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entire channel set to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This file contains the implementation of the TableLlama model.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from transformers import LlamaConfig, LlamaModel, LlamaForCausalLM\n",
    "from transformers.modeling_rope_utils import ROPE_INIT_FUNCTIONS\n",
    "\n",
    "import transformers.utils.logging as logging\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "\"\"\"\n",
    "TableLlamaRotaryEmbedding\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class TableLlamaRotaryEmbeddingZeroChannel(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: TableLlamaConfig,\n",
    "        device=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = config\n",
    "        self.rope_type = getattr(config.rope_scaling, \"rope_type\", \"llama3\")\n",
    "    \n",
    "        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n",
    "\n",
    "        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n",
    "        \n",
    "        # Table Llama specific initialization\n",
    "        self.rope_table_llama = getattr(self.config, \"rope_table_llama\")\n",
    "\n",
    "        x_channels_start = self.rope_table_llama[\"x_channels_start\"]\n",
    "        x_channels_end = self.rope_table_llama[\"x_channels_end\"]\n",
    "        x_channels_step = self.rope_table_llama[\"x_channels_step\"]\n",
    "        y_channels_start = self.rope_table_llama[\"y_channels_start\"]\n",
    "        y_channels_end = self.rope_table_llama[\"y_channels_end\"]\n",
    "        y_channels_step = self.rope_table_llama[\"y_channels_step\"]\n",
    "        line_length = self.rope_table_llama[\"line_length\"]\n",
    "        \n",
    "        if line_length is None:\n",
    "            # Set a large number to avoid the RoPE effect\n",
    "            line_length = 10**8\n",
    "\n",
    "            \n",
    "        if x_channels_end is None:\n",
    "            x_channels_start = 10**8\n",
    "            x_channels_end = 10**8\n",
    "            x_channels_step = 10**8\n",
    "        else:\n",
    "            if x_channels_step is None or x_channels_start is None:\n",
    "                raise ValueError(\"You have set x_channels_end but not x_channels_step or x_channels_start\")\n",
    "          \n",
    "        if y_channels_end is None:\n",
    "            y_channels_start = 10**8\n",
    "            y_channels_end = 10**8\n",
    "            y_channels_step = 10**8\n",
    "        else:\n",
    "            if y_channels_step is None or y_channels_start is None:\n",
    "                raise ValueError(\"You have set y_channels_end but not y_channels_step or y_channels_start\")\n",
    "\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "        \n",
    "        self.num_channels = inv_freq.shape[0]\n",
    "        self.line_length = line_length\n",
    "        self.x_channels_start = x_channels_start\n",
    "        self.x_channels_end = x_channels_end\n",
    "        self.x_channels_step = x_channels_step\n",
    "        self.y_channels_start = y_channels_start\n",
    "        self.y_channels_end = y_channels_end\n",
    "        self.y_channels_step = y_channels_step\n",
    "        \n",
    "        \n",
    "        \n",
    "  \n",
    "    @torch.no_grad()\n",
    "    def forward(self, x, position_ids):\n",
    "        if \"dynamic\" in self.rope_type:\n",
    "            self._dynamic_frequency_update(position_ids, device=x.device)\n",
    "\n",
    "        # Core RoPE block\n",
    "        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1) # (1, num_channels, 1)\n",
    "        position_ids_expanded = position_ids[:, None, :].float() # (batch_size, 1, seq_len)\n",
    "        position_ids_expanded = position_ids_expanded.repeat(1, self.num_channels, 1) # (batch_size, num_channels, seq_len)\n",
    "        \n",
    "        x_position_ids = position_ids_expanded % self.line_length\n",
    "        y_position_ids = position_ids_expanded // self.line_length\n",
    "        \n",
    "        # Replace the position_ids_expanded with x_position_ids and y_position_ids\n",
    "        x_start = self.x_channels_start\n",
    "        x_end = self.x_channels_end\n",
    "        x_step = self.x_channels_step\n",
    "        y_start = self.y_channels_start\n",
    "        y_end = self.y_channels_end\n",
    "        y_step = self.y_channels_step\n",
    "        \n",
    "        # Specify the channel index to set to zero (e.g., channel 1)\n",
    "        # Set the entire channel to zero\n",
    "        position_ids_expanded[:, :, :] = 0\n",
    "\n",
    "        # position_ids_expanded[:, x_start:x_end:x_step, :] = x_position_ids[:, x_start:x_end:x_step, :]\n",
    "        # position_ids_expanded[:, y_start:y_end:y_step, :] = y_position_ids[:, y_start:y_end:y_step, :]\n",
    "        \n",
    "        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n",
    "        device_type = x.device.type\n",
    "        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n",
    "        with torch.autocast(device_type=device_type, enabled=False):\n",
    "                        \n",
    "            freqs = position_ids_expanded * inv_freq_expanded # (batch_size, num_channels, seq_len)\n",
    "            freqs = freqs.transpose(1, 2) # (batch_size, seq_len, num_channels)\n",
    "            \n",
    "            emb = torch.cat((freqs, freqs), dim=-1)\n",
    "\n",
    "            cos = emb.cos()\n",
    "            sin = emb.sin()\n",
    "        \n",
    "\n",
    "        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n",
    "        cos = cos * self.attention_scaling # (batch_size, seq_len, dim)\n",
    "        sin = sin * self.attention_scaling # (batch_size, seq_len, dim)\n",
    "\n",
    "        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n",
    "\n",
    "\"\"\"\n",
    "TableLlamaModel\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class TableLlamaModelZeroChannel(LlamaModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.rotary_emb = TableLlamaRotaryEmbeddingZeroChannel(config)\n",
    "\n",
    "\n",
    "class TableLlamaForCausalLMZeroChannel(LlamaForCausalLM):\n",
    "    def __init__(self, config: TableLlamaConfig):\n",
    "        # Change the config class to TableLlamaConfig\n",
    "        super().__init__(config)\n",
    "        # if getattr(config, \"rope_table_llama\", None) is None:\n",
    "        #     logger.warning(\"[TableLlamaForCausalLM] `rope_table_llama` is None. Using default values.\")\n",
    "        #     config.rope_table_llama = DEFAULT_ROPE_TABLE_LLAMA\n",
    "        \n",
    "        self.model = TableLlamaModelZeroChannel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TableLlamaForCausalLMZeroChannel(\n",
       "  (model): TableLlamaModelZeroChannel(\n",
       "    (embed_tokens): Embedding(128256, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear8bitLt(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear8bitLt(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear8bitLt(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear8bitLt(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear8bitLt(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear8bitLt(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear8bitLt(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): TableLlamaRotaryEmbeddingZeroChannel()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_llama_model = TableLlamaForCausalLMZeroChannel.from_pretrained(\n",
    "    MODEL_NAME, \n",
    "    quantization_config=bnb_config, \n",
    "    device_map=\"auto\",\n",
    "    config=table_llama_config\n",
    ")\n",
    "\n",
    "table_llama_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Gaussian noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TableLlamaRotaryEmbedding\n",
    "\"\"\"\n",
    "\n",
    "class TableLlamaRotaryEmbeddingNoise(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: TableLlamaConfig,\n",
    "        device=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = config\n",
    "        self.rope_type = getattr(config.rope_scaling, \"rope_type\", \"llama3\")\n",
    "    \n",
    "        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n",
    "\n",
    "        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n",
    "        \n",
    "        # Table Llama specific initialization\n",
    "        self.rope_table_llama = getattr(self.config, \"rope_table_llama\")\n",
    "\n",
    "        x_channels_start = self.rope_table_llama[\"x_channels_start\"]\n",
    "        x_channels_end = self.rope_table_llama[\"x_channels_end\"]\n",
    "        x_channels_step = self.rope_table_llama[\"x_channels_step\"]\n",
    "        y_channels_start = self.rope_table_llama[\"y_channels_start\"]\n",
    "        y_channels_end = self.rope_table_llama[\"y_channels_end\"]\n",
    "        y_channels_step = self.rope_table_llama[\"y_channels_step\"]\n",
    "        line_length = self.rope_table_llama[\"line_length\"]\n",
    "        \n",
    "        if line_length is None:\n",
    "            # Set a large number to avoid the RoPE effect\n",
    "            line_length = 10**8\n",
    "\n",
    "            \n",
    "        if x_channels_end is None:\n",
    "            x_channels_start = 10**8\n",
    "            x_channels_end = 10**8\n",
    "            x_channels_step = 10**8\n",
    "        else:\n",
    "            if x_channels_step is None or x_channels_start is None:\n",
    "                raise ValueError(\"You have set x_channels_end but not x_channels_step or x_channels_start\")\n",
    "          \n",
    "        if y_channels_end is None:\n",
    "            y_channels_start = 10**8\n",
    "            y_channels_end = 10**8\n",
    "            y_channels_step = 10**8\n",
    "        else:\n",
    "            if y_channels_step is None or y_channels_start is None:\n",
    "                raise ValueError(\"You have set y_channels_end but not y_channels_step or y_channels_start\")\n",
    "\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "        \n",
    "        self.num_channels = inv_freq.shape[0]\n",
    "        self.line_length = line_length\n",
    "        self.x_channels_start = x_channels_start\n",
    "        self.x_channels_end = x_channels_end\n",
    "        self.x_channels_step = x_channels_step\n",
    "        self.y_channels_start = y_channels_start\n",
    "        self.y_channels_end = y_channels_end\n",
    "        self.y_channels_step = y_channels_step\n",
    "        \n",
    "        \n",
    "        \n",
    "  \n",
    "    @torch.no_grad()\n",
    "    def forward(self, x, position_ids):\n",
    "        if \"dynamic\" in self.rope_type:\n",
    "            self._dynamic_frequency_update(position_ids, device=x.device)\n",
    "\n",
    "        # Core RoPE block\n",
    "        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1) # (1, num_channels, 1)\n",
    "        position_ids_expanded = position_ids[:, None, :].float() # (batch_size, 1, seq_len)\n",
    "        position_ids_expanded = position_ids_expanded.repeat(1, self.num_channels, 1) # (batch_size, num_channels, seq_len)\n",
    "        \n",
    "        x_position_ids = position_ids_expanded % self.line_length\n",
    "        y_position_ids = position_ids_expanded // self.line_length\n",
    "        \n",
    "        # Replace the position_ids_expanded with x_position_ids and y_position_ids\n",
    "        x_start = self.x_channels_start\n",
    "        x_end = self.x_channels_end\n",
    "        x_step = self.x_channels_step\n",
    "        y_start = self.y_channels_start\n",
    "        y_end = self.y_channels_end\n",
    "        y_step = self.y_channels_step\n",
    "        \n",
    "        # Specify the channel index to set to zero (e.g., channel 1)\n",
    "        # channel_index = 0\n",
    "        noise = np.random.normal(0, 0.1, 1)\n",
    "        position_ids_expanded[:, :, :] += noise\n",
    "\n",
    "        # position_ids_expanded[:, x_start:x_end:x_step, :] = x_position_ids[:, x_start:x_end:x_step, :]\n",
    "        # position_ids_expanded[:, y_start:y_end:y_step, :] = y_position_ids[:, y_start:y_end:y_step, :]\n",
    "        \n",
    "        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n",
    "        device_type = x.device.type\n",
    "        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n",
    "        with torch.autocast(device_type=device_type, enabled=False):\n",
    "                        \n",
    "            freqs = position_ids_expanded * inv_freq_expanded # (batch_size, num_channels, seq_len)\n",
    "            freqs = freqs.transpose(1, 2) # (batch_size, seq_len, num_channels)\n",
    "            \n",
    "            emb = torch.cat((freqs, freqs), dim=-1)\n",
    "\n",
    "            cos = emb.cos()\n",
    "            sin = emb.sin()\n",
    "        \n",
    "\n",
    "        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n",
    "        cos = cos * self.attention_scaling # (batch_size, seq_len, dim)\n",
    "        sin = sin * self.attention_scaling # (batch_size, seq_len, dim)\n",
    "\n",
    "        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n",
    "\n",
    "\"\"\"\n",
    "TableLlamaModel\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class TableLlamaModelNoise(LlamaModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.rotary_emb = TableLlamaRotaryEmbeddingZeroChannel(config)\n",
    "\n",
    "\n",
    "class TableLlamaForCausalNoise(LlamaForCausalLM):\n",
    "    def __init__(self, config: TableLlamaConfig):\n",
    "        # Change the config class to TableLlamaConfig\n",
    "        super().__init__(config)\n",
    "        # if getattr(config, \"rope_table_llama\", None) is None:\n",
    "        #     logger.warning(\"[TableLlamaForCausalLM] `rope_table_llama` is None. Using default values.\")\n",
    "        #     config.rope_table_llama = DEFAULT_ROPE_TABLE_LLAMA\n",
    "        \n",
    "        self.model = TableLlamaModelZeroChannel(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    HfArgumentParser, \n",
    "    PreTrainedTokenizerFast, \n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "import torch\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "from parsers.argument_classes import DatasetArguments, ModelArguments, TrainingArguments, GenerationArguments\n",
    "from utils.datasets_loader import load_datasets\n",
    "from collators.data_collator_for_grid_tokenization import DataCollatorForGridTokenization\n",
    "\n",
    "def main():\n",
    "    parser = HfArgumentParser((DatasetArguments, ModelArguments, TrainingArguments, GenerationArguments))\n",
    "    if len(sys.argv) == 2 and sys.argv[1].endswith(\".yaml\"):\n",
    "        dataset_args, model_args, training_args, generation_args = parser.parse_yaml_file(sys.argv[1])\n",
    "    else:\n",
    "        args_list = [\"--dataset_root_dir\", \"../datasets\", \"--dataset_names\", \"wtq\" ,\"--test_max_samples_for_each_dataset\", \"480\", \"--load_in_4bit\", \"True\",\n",
    "                    \"--batch_size\", \"4\", \"--set_channel_zero\", \"False\", \"--add_channel_noise\", \"True\"]\n",
    "        dataset_args, model_args, training_args, generation_args = parser.parse_args_into_dataclasses(args_list)\n",
    "    print(model_args)\n",
    "    print(training_args)\n",
    "            \n",
    "    # Tokenizer\n",
    "    tokenizer = PreTrainedTokenizerFast.from_pretrained(model_args.model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    \n",
    "    # Model\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=model_args.load_in_4bit,\n",
    "        load_in_8bit=model_args.load_in_8bit,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16 if model_args.load_in_4bit else None,\n",
    "        bnb_4bit_use_double_quant=model_args.load_in_4bit,\n",
    "    )\n",
    "    # TableLlama\n",
    "    table_llama_config = TableLlamaConfig.from_pretrained(model_args.model_name)\n",
    "    table_llama_config.rope_table_llama = {\n",
    "        \"line_length\": model_args.line_length,\n",
    "        \"x_channels_start\": model_args.x_channels_start,\n",
    "        \"x_channels_end\": model_args.x_channels_end,\n",
    "        \"x_channels_step\": model_args.x_channels_step,\n",
    "        \"y_channels_start\": model_args.y_channels_start,\n",
    "        \"y_channels_end\": model_args.y_channels_end,\n",
    "        \"y_channels_step\": model_args.y_channels_step,\n",
    "    }\n",
    "    # Replace with Model experimenting\n",
    "    if (model_args.set_channel_zero): \n",
    "        model = TableLlamaForCausalLMZeroChannel.from_pretrained(\n",
    "            model_args.model_name, \n",
    "            quantization_config=bnb_config if model_args.load_in_4bit or model_args.load_in_8bit else None,\n",
    "            device_map=\"auto\",\n",
    "            config=table_llama_config\n",
    "        )\n",
    "    elif (model_args.add_channel_noise): \n",
    "        model = TableLlamaForCausalNoise.from_pretrained(\n",
    "            model_args.model_name, \n",
    "            quantization_config=bnb_config if model_args.load_in_4bit or model_args.load_in_8bit else None,\n",
    "            device_map=\"auto\",\n",
    "            config=table_llama_config\n",
    "        )\n",
    "    model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "    model.generation_config.eos_token_id = tokenizer.eos_token_id\n",
    "    # Load adapter\n",
    "    if model_args.adapter_path:\n",
    "        model.load_adapter(model_args.adapter_path)\n",
    "    \n",
    "    # Load datasets\n",
    "    def filter_function(example):\n",
    "        if dataset_args.max_table_row_num is not None and example[\"table_row_num\"] > dataset_args.max_table_row_num:\n",
    "            return False\n",
    "        if dataset_args.max_table_width is not None and example[\"table_width\"] > dataset_args.max_table_width:\n",
    "            return False\n",
    "        return True\n",
    "    datasets = load_datasets(dataset_args, filter_function=filter_function)\n",
    "    \n",
    "    # Data collator\n",
    "    data_collator = DataCollatorForGridTokenization(\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=training_args.max_seq_length,\n",
    "        is_train=False,\n",
    "        is_grid_tokenization=model_args.line_length is not None,\n",
    "        line_length=model_args.line_length if model_args.line_length is not None else 64,\n",
    "    )\n",
    "    \n",
    "    # Inference loop\n",
    "    pred_dataloader = DataLoader(\n",
    "        datasets[\"test\"],\n",
    "        collate_fn=data_collator,\n",
    "        batch_size=training_args.batch_size,\n",
    "    )\n",
    "    \n",
    "    # Predict\n",
    "    \n",
    "    predictions = []\n",
    "    for idx, batch in enumerate(tqdm(pred_dataloader)):\n",
    "        with torch.no_grad():\n",
    "            input_length = batch[\"input_ids\"].size(1)\n",
    "            # Move the batch to the device\n",
    "            batch = {k: v.to(model.device) for k, v in batch.items()}\n",
    "            outputs = model.generate(\n",
    "                **batch, \n",
    "                max_new_tokens=generation_args.max_new_tokens,\n",
    "                do_sample=generation_args.do_sample,\n",
    "                top_k=generation_args.top_k,\n",
    "                top_p=generation_args.top_p,\n",
    "                temperature=generation_args.temperature,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "            output_strings = tokenizer.batch_decode(outputs[:, input_length:], skip_special_tokens=False)\n",
    "            predictions.extend(output_strings)\n",
    "    \n",
    "    # Create a new column for predictions\n",
    "    df = datasets[\"test\"].to_pandas()\n",
    "    df[\"raw_predictions\"] = predictions\n",
    "    \n",
    "    def clean_predictions(predictions):\n",
    "        return [pred.replace(\"<|eot_id|>\", \"\").replace(\"Answer:\", \"\").strip() for pred in predictions]\n",
    "    \n",
    "    df[\"predictions\"] = clean_predictions(df[\"raw_predictions\"])\n",
    "    df[\"correct\"] = df[\"answer\"] == df[\"predictions\"]\n",
    "    \n",
    "    print(f\"Base model: {model_args.model_name}\")\n",
    "    print(f\"Adapter: {model_args.adapter_path}\")\n",
    "    print(f\"Total samples: {df.shape[0]}\")\n",
    "\n",
    "    if \"self_generated\" in dataset_args.dataset_names:\n",
    "        \n",
    "        # Count accuracy for each task and direction\n",
    "        list_item_row_total = df[(df[\"task\"] == \"list_items\") & (df[\"direction\"] == \"row\")].shape[0]\n",
    "        list_item_col_total = df[(df[\"task\"] == \"list_items\") & (df[\"direction\"] == \"column\")].shape[0]\n",
    "        arithmetic_row_total = df[(df[\"task\"] == \"arithmetic\") & (df[\"direction\"] == \"row\")].shape[0]\n",
    "        arithmetic_col_total = df[(df[\"task\"] == \"arithmetic\") & (df[\"direction\"] == \"column\")].shape[0]\n",
    "        \n",
    "        list_item_row_correct = df[(df[\"task\"] == \"list_items\") & (df[\"direction\"] == \"row\") & (df[\"correct\"])].shape[0] \n",
    "        list_item_col_correct = df[(df[\"task\"] == \"list_items\") & (df[\"direction\"] == \"column\") & (df[\"correct\"])].shape[0] \n",
    "        arithmetic_row_correct = df[(df[\"task\"] == \"arithmetic\") & (df[\"direction\"] == \"row\") & (df[\"correct\"])].shape[0] \n",
    "        arithmetic_col_correct = df[(df[\"task\"] == \"arithmetic\") & (df[\"direction\"] == \"column\") & (df[\"correct\"])].shape[0] \n",
    "        \n",
    "        self_generated_total = list_item_row_total + list_item_col_total + arithmetic_row_total + arithmetic_col_total\n",
    "        self_generated_correct = list_item_row_correct + list_item_col_correct + arithmetic_row_correct + arithmetic_col_correct\n",
    "        \n",
    "        print(f\"List item row correct: {list_item_row_correct} / {list_item_row_total} = {list_item_row_correct / list_item_row_total * 100:.2f}%\")\n",
    "        print(f\"List item column correct: {list_item_col_correct} / {list_item_col_total} = {list_item_col_correct / list_item_col_total * 100:.2f}%\")\n",
    "        print(f\"Arithmetic row correct: {arithmetic_row_correct} / {arithmetic_row_total} = {arithmetic_row_correct / arithmetic_row_total * 100:.2f}%\")\n",
    "        print(f\"Arithmetic column correct: {arithmetic_col_correct} / {arithmetic_col_total} = {arithmetic_col_correct / arithmetic_col_total * 100:.2f}%\")\n",
    "        print(f\"Self-generated correct: {self_generated_correct} / {self_generated_total} = {self_generated_correct / self_generated_total * 100:.2f}%\")\n",
    "    \n",
    "    if \"wtq\" in dataset_args.dataset_names:\n",
    "        wtq_total = df[df[\"task\"] == \"wtq\"].shape[0]\n",
    "        wtq_correct = df[(df[\"task\"] == \"wtq\") & (df[\"correct\"])].shape[0]\n",
    "            \n",
    "        print(f\"WTQ correct: {wtq_correct} / {wtq_total} = {wtq_correct / wtq_total * 100:.2f}%\")\n",
    "    \n",
    "    total_correct = df[\"correct\"].sum()\n",
    "    total_total = df.shape[0]\n",
    "    \n",
    "    print(f\"Total correct: {total_correct} / {total_total} = {total_correct / total_total * 100:.2f}%\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Save it to the adapter path\n",
    "    if model_args.adapter_path:\n",
    "        output_path = os.path.join(model_args.adapter_path, f\"predictions.csv\")\n",
    "    else:\n",
    "        # Create the output directory if not exists\n",
    "        os.makedirs(training_args.output_dir, exist_ok=True)\n",
    "        output_path = os.path.join(training_args.output_dir, f\"{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}_predictions.csv\")\n",
    "        \n",
    "    df.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelArguments(model_name='meta-llama/Llama-3.2-1B-Instruct', adapter_path='', load_in_8bit=False, load_in_4bit=True, line_length=None, x_channels_start=None, x_channels_end=None, x_channels_step=None, y_channels_start=None, y_channels_end=None, y_channels_step=None, set_channel_zero=False, add_channel_noise=True)\n",
      "TrainingArguments(output_dir='./outputs', gradient_accumulation_steps=1, batch_size=4, num_train_epochs=1, save_total_limit=3, save_steps=100, logging_steps=10, eval_steps=200, max_seq_length=1024, dry_run=False, run_id_prefix='run', wandb_entity=None, wandb_project=None, hf_organization='cs230-table-llama', push_to_hub=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▍                                                                                  | 2/120 [00:03<03:13,  1.64s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# With zero channel\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m main()\n",
      "Cell \u001b[0;32mIn[7], line 105\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# Move the batch to the device\u001b[39;00m\n\u001b[1;32m    104\u001b[0m batch \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m--> 105\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbatch, \n\u001b[1;32m    107\u001b[0m     max_new_tokens\u001b[38;5;241m=\u001b[39mgeneration_args\u001b[38;5;241m.\u001b[39mmax_new_tokens,\n\u001b[1;32m    108\u001b[0m     do_sample\u001b[38;5;241m=\u001b[39mgeneration_args\u001b[38;5;241m.\u001b[39mdo_sample,\n\u001b[1;32m    109\u001b[0m     top_k\u001b[38;5;241m=\u001b[39mgeneration_args\u001b[38;5;241m.\u001b[39mtop_k,\n\u001b[1;32m    110\u001b[0m     top_p\u001b[38;5;241m=\u001b[39mgeneration_args\u001b[38;5;241m.\u001b[39mtop_p,\n\u001b[1;32m    111\u001b[0m     temperature\u001b[38;5;241m=\u001b[39mgeneration_args\u001b[38;5;241m.\u001b[39mtemperature,\n\u001b[1;32m    112\u001b[0m     pad_token_id\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39meos_token_id\n\u001b[1;32m    113\u001b[0m )\n\u001b[1;32m    114\u001b[0m output_strings \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(outputs[:, input_length:], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    115\u001b[0m predictions\u001b[38;5;241m.\u001b[39mextend(output_strings)\n",
      "File \u001b[0;32m~/mambaforge/envs/cs230/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/mambaforge/envs/cs230/lib/python3.11/site-packages/transformers/generation/utils.py:2215\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2207\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2208\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2209\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2210\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2211\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2212\u001b[0m     )\n\u001b[1;32m   2214\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2215\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample(\n\u001b[1;32m   2216\u001b[0m         input_ids,\n\u001b[1;32m   2217\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[1;32m   2218\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[1;32m   2219\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[1;32m   2220\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[1;32m   2221\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[1;32m   2222\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2223\u001b[0m     )\n\u001b[1;32m   2225\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2226\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2227\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2228\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2229\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2234\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2235\u001b[0m     )\n",
      "File \u001b[0;32m~/mambaforge/envs/cs230/lib/python3.11/site-packages/transformers/generation/utils.py:3206\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3203\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   3205\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 3206\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   3208\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3209\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3210\u001b[0m     outputs,\n\u001b[1;32m   3211\u001b[0m     model_kwargs,\n\u001b[1;32m   3212\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3213\u001b[0m )\n",
      "File \u001b[0;32m~/mambaforge/envs/cs230/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/mambaforge/envs/cs230/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/cs230/lib/python3.11/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/mambaforge/envs/cs230/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:1190\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[1;32m   1187\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1190\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[1;32m   1191\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1192\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   1193\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   1194\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m   1195\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m   1196\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m   1197\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1198\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1199\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1200\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m   1201\u001b[0m )\n\u001b[1;32m   1203\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/mambaforge/envs/cs230/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/mambaforge/envs/cs230/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/cs230/lib/python3.11/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/mambaforge/envs/cs230/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:945\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    933\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    934\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    935\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    942\u001b[0m         position_embeddings,\n\u001b[1;32m    943\u001b[0m     )\n\u001b[1;32m    944\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 945\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[1;32m    946\u001b[0m         hidden_states,\n\u001b[1;32m    947\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mcausal_mask,\n\u001b[1;32m    948\u001b[0m         position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m    949\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m    950\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    951\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m    952\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m    953\u001b[0m         position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[1;32m    954\u001b[0m     )\n\u001b[1;32m    956\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/mambaforge/envs/cs230/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/mambaforge/envs/cs230/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/cs230/lib/python3.11/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/mambaforge/envs/cs230/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:676\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    673\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    675\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 676\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[1;32m    677\u001b[0m     hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m    678\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    679\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m    680\u001b[0m     past_key_value\u001b[38;5;241m=\u001b[39mpast_key_value,\n\u001b[1;32m    681\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    682\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m    683\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m    684\u001b[0m     position_embeddings\u001b[38;5;241m=\u001b[39mposition_embeddings,\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    686\u001b[0m )\n\u001b[1;32m    687\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    689\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/cs230/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/mambaforge/envs/cs230/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/cs230/lib/python3.11/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/mambaforge/envs/cs230/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:582\u001b[0m, in \u001b[0;36mLlamaSdpaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;66;03m# sin and cos are specific to RoPE models; cache_position needed for the static cache\u001b[39;00m\n\u001b[1;32m    581\u001b[0m     cache_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msin\u001b[39m\u001b[38;5;124m\"\u001b[39m: sin, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcos\u001b[39m\u001b[38;5;124m\"\u001b[39m: cos, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_position\u001b[39m\u001b[38;5;124m\"\u001b[39m: cache_position}\n\u001b[0;32m--> 582\u001b[0m     key_states, value_states \u001b[38;5;241m=\u001b[39m past_key_value\u001b[38;5;241m.\u001b[39mupdate(key_states, value_states, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_idx, cache_kwargs)\n\u001b[1;32m    584\u001b[0m key_states \u001b[38;5;241m=\u001b[39m repeat_kv(key_states, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_key_value_groups)\n\u001b[1;32m    585\u001b[0m value_states \u001b[38;5;241m=\u001b[39m repeat_kv(value_states, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_key_value_groups)\n",
      "File \u001b[0;32m~/mambaforge/envs/cs230/lib/python3.11/site-packages/transformers/cache_utils.py:448\u001b[0m, in \u001b[0;36mDynamicCache.update\u001b[0;34m(self, key_states, value_states, layer_idx, cache_kwargs)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_cache[layer_idx] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_cache[layer_idx], key_states], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 448\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_cache[layer_idx] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_cache[layer_idx], value_states], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_cache[layer_idx], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_cache[layer_idx]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# With zero channel\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelArguments(model_name='meta-llama/Llama-3.2-1B-Instruct', adapter_path='', load_in_8bit=False, load_in_4bit=True, line_length=None, x_channels_start=None, x_channels_end=None, x_channels_step=None, y_channels_start=None, y_channels_end=None, y_channels_step=None, set_channel_zero=False, add_channel_noise=True)\n",
      "TrainingArguments(output_dir='./outputs', gradient_accumulation_steps=1, batch_size=4, num_train_epochs=1, save_total_limit=3, save_steps=100, logging_steps=10, eval_steps=200, max_seq_length=1024, dry_run=False, run_id_prefix='run', wandb_entity=None, wandb_project=None, hf_organization='cs230-table-llama', push_to_hub=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████████████████████████████████████████████████████████████████████▍         | 106/120 [03:27<00:45,  3.27s/it]"
     ]
    }
   ],
   "source": [
    "# With gaussian noise\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
