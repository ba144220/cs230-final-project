{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "from typing import List, Dict, Any, Union\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer, SFTConfig, DataCollatorForCompletionOnlyLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS_DIR = \"../datasets\"\n",
    "DATASET_NAME = \"self_generated\"\n",
    "\n",
    "SYSTEM_PROMPT = \"You are a helpful assistant that answers questions about the table. You only answer the question right after 'Answer: '\"\n",
    "ASSISTANT_PROMPT = \"Answer: \"\n",
    "SHUFFLE_SEED = 42\n",
    "\n",
    "USER_PROMPT_ORDER = [\"table\", \"question\"]\n",
    "TABLE_EXTENSION = \"csv\"\n",
    "\n",
    "MODEL_NAME = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "MAX_SEQ_LENGTH = 1024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.path.join(DATASETS_DIR, DATASET_NAME)\n",
    "dataset = load_dataset(\"csv\", data_files={\n",
    "    \"train\": os.path.join(dataset_path, \"data\", \"train.csv\"),\n",
    "    \"test\": os.path.join(dataset_path, \"data\", \"test.csv\"),\n",
    "    \"validation\": os.path.join(dataset_path, \"data\", \"val.csv\")\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly shuffle the dataset\n",
    "# dataset = dataset.shuffle(seed=SHUFFLE_SEED)\n",
    "\n",
    "# Pick the first 100 examples\n",
    "dataset[\"train\"] = dataset[\"train\"].select(range(480))\n",
    "dataset[\"validation\"] = dataset[\"validation\"].select(range(80))\n",
    "dataset[\"test\"] = dataset[\"test\"].select(range(80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table(context: str):\n",
    "    context = re.sub(f\".csv$\", \"\", context)\n",
    "    with open(os.path.join(dataset_path, context + \".\" + TABLE_EXTENSION), \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def preprocess_single_example_to_string(example):\n",
    "    table = get_table(example[\"context\"])\n",
    "    example[\"table\"] = table\n",
    "    \n",
    "    user_prompt = \"\\n\".join([example[col_name] for col_name in USER_PROMPT_ORDER])\n",
    "    \n",
    "    assistant_response = ASSISTANT_PROMPT + str(example[\"answer\"])\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "        {\"role\": \"assistant\", \"content\": assistant_response}\n",
    "    ]\n",
    "    \n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "    example[\"input_string\"] = text\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(preprocess_single_example_to_string, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, quantization_config=quantization_config, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_example(example):\n",
    "    return example[\"input_string\"]\n",
    "\n",
    "def last_occurrence_indices(tensor, X):\n",
    "    # tensor: 2D tensor of shape B x L\n",
    "    # X: the integer value to find in the tensor\n",
    "\n",
    "    # Create a boolean mask where elements equal to X are True\n",
    "    mask = (tensor == X)  # Shape: B x L\n",
    "\n",
    "    # Reverse the mask along the sequence dimension (dimension 1)\n",
    "    reversed_mask = torch.flip(mask, dims=[1])  # Shape: B x L\n",
    "\n",
    "    # Find the index of the first occurrence of True in the reversed mask\n",
    "    # Convert boolean mask to float to use argmax (True becomes 1.0, False becomes 0.0)\n",
    "    idx_in_reversed = reversed_mask.float().argmax(dim=1)  # Shape: B\n",
    "\n",
    "    # Calculate the last occurrence index in the original tensor\n",
    "    last_indices = tensor.size(1) - idx_in_reversed - 1  # Shape: B\n",
    "\n",
    "    # Handle rows where X does not occur\n",
    "    # If X does not occur in a row, the entire mask row is False, and argmax returns 0\n",
    "    # We need to set last_indices for these rows to -1 or any invalid index as per your requirements\n",
    "    has_X = mask.any(dim=1)  # Shape: B (True if X is in the row)\n",
    "    last_indices[~has_X] = -1  # Set to -1 where X does not occur\n",
    "\n",
    "    return last_indices.unsqueeze(1)  # Shape: B x 1\n",
    "\n",
    "def data_collator_for_assistant_completion_only(examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict[str, Any]:\n",
    "    \n",
    "    input_strings = [example[\"input_string\"] for example in examples]\n",
    "    batch = tokenizer(input_strings, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_SEQ_LENGTH, add_special_tokens=False)\n",
    "    batch[\"labels\"] = batch[\"input_ids\"].clone()\n",
    "    \n",
    "    # Find the index of the last <|end_header_id|> (128007)\n",
    "    # And set the labels before the last <|end_header_id|> to -100\n",
    "    last_end_header_id_indices = last_occurrence_indices(batch[\"input_ids\"], 128007)\n",
    "    batch[\"labels\"][range(len(batch[\"labels\"])), :last_end_header_id_indices+2] = -100\n",
    "    \n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuchi/mambaforge/envs/cs230/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:537: UserWarning: You passed `remove_unused_columns=False` on a non-packed dataset. This might create some issues with the default collator and yield to errors. If you want to inspect dataset other columns (in this case ['answer', 'size', 'direction', 'input_string', 'question', 'table', 'id', 'context', 'task']), you can subclass `DataCollatorForLanguageModeling` in case you used the default collator and create your own data collator in order to inspect the unused dataset columns.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e1d37695199447dbbed9aca86ec3e8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/480 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "258638cce3cf4a1db8b5768e02a3ef0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/80 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "peft_config = LoraConfig(\n",
    "    task_type=\"SEQ_2_SEQ_LM\",\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "sft_config = SFTConfig(\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    output_dir=\"../outputs/sft_trainer_collator\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=1,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    bf16=True,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "sft_trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=sft_config,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    peft_config=peft_config,\n",
    "    data_collator=data_collator_for_assistant_completion_only,\n",
    "    formatting_func=format_example,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mb08097\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/work/yuchi/stanford/2024-fall/cs230/cs230-final-project/experiments/wandb/run-20241110_151327-v1c1hmyp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/b08097/huggingface/runs/v1c1hmyp' target=\"_blank\">../outputs/sft_trainer_collator</a></strong> to <a href='https://wandb.ai/b08097/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/b08097/huggingface' target=\"_blank\">https://wandb.ai/b08097/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/b08097/huggingface/runs/v1c1hmyp' target=\"_blank\">https://wandb.ai/b08097/huggingface/runs/v1c1hmyp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuchi/mambaforge/envs/cs230/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/home/yuchi/mambaforge/envs/cs230/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='480' max='480' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [480/480 00:46, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.162000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.891700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.382800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.553000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.325200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.323400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.615000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.571100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.807900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.438400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.486800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.693000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.801400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.067000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.767800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.059800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.737500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.737700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.205100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.720900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.517900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.210700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.805300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.398100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.857400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.320900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.699500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.614400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.310500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.726500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.442900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.367900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>1.406200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.508900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.328600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.714800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.709700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.545600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.743200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.770500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.158900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.698000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.751100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.505200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.254400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.745000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.159200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.703600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=480, training_loss=0.7567129435638587, metrics={'train_runtime': 48.4682, 'train_samples_per_second': 9.903, 'train_steps_per_second': 9.903, 'total_flos': 708291403776000.0, 'train_loss': 0.7567129435638587, 'epoch': 1.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sft_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[128006, 78191, 128007]], 'attention_mask': [[1, 1, 1]]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer([\"<|start_header_id|>assistant<|end_header_id|>\"], add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|begin_of_text|>', '<|eot_id|>']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.all_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 605, 4723, 220, 2366, 19, 271, 2675, 527, 264, 11190, 18328, 430, 11503, 4860, 922, 279, 2007, 13, 1472, 1193, 4320, 279, 3488, 1314, 1306, 364, 16533, 25, 364, 128009, 128006, 882, 128007, 271, 11, 6255, 220, 16, 11, 6255, 220, 17, 11, 6255, 220, 18, 11, 6255, 220, 19, 198, 3179, 220, 16, 11, 3971, 11, 6083, 11, 975, 11, 6028, 198, 3179, 220, 17, 11, 1399, 11, 508, 11, 6086, 11, 4218, 198, 3179, 220, 18, 11, 5728, 11, 5728, 11, 4044, 11, 1484, 198, 3179, 220, 19, 11, 1419, 11, 17, 11, 1691, 11, 4103, 271, 3923, 374, 279, 7340, 315, 279, 2819, 304, 11035, 220, 16, 30, 128009, 128006, 78191, 128007, 271, 16533, 25, 220, 6083, 128009]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer([dataset[\"train\"][0][\"input_string\"]], add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2024\\n\\nYou are a helpful assistant that answers questions about the table. You only answer the question right after 'Answer: '<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n,Col 1,Col 2,Col 3,Col 4\\nRow 1,51,92,14,71\\nRow 2,60,20,82,86\\nRow 3,74,74,87,99\\nRow 4,23,2,21,52\\n\\nWhat is the maximum of the values in Row 1?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nAnswer: 92<|eot_id|>\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0][\"input_string\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[128006, 78191, 128007, 271, 16533, 25, 220]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer([\"<|start_header_id|>assistant<|end_header_id|>\\n\\nAnswer: \"], add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Answer: Horton Smith<|eot_id|>'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([ 16533,     25,  95695,   9259, 128009])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs230",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
