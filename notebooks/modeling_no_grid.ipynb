{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaForCausalLM, BitsAndBytesConfig, AutoTokenizer\n",
    "\n",
    "# quantization config\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    ")\n",
    "\n",
    "MODEL_NAME = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "llama_model = LlamaForCausalLM.from_pretrained(MODEL_NAME, quantization_config=quantization_config, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "from models.modeling_no_grid import LlamaNoGridForCausalLM\n",
    "from models.modeling_table_llama import TableLlamaConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "if \"no_grid_model\" in globals():\n",
    "    del no_grid_model\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "config = TableLlamaConfig.from_pretrained(MODEL_NAME)\n",
    "config.rope_table_llama = {\n",
    "    \"x_channels_start\": 0,\n",
    "    \"x_channels_end\": 128,\n",
    "    \"x_channels_step\": 2,\n",
    "    \"y_channels_start\": 1,\n",
    "    \"y_channels_end\": 128,\n",
    "    \"y_channels_step\": 2,\n",
    "}\n",
    "\n",
    "no_grid_model = LlamaNoGridForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, \n",
    "    quantization_config=quantization_config, \n",
    "    device_map=\"auto\",\n",
    "    config=config\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[128000, 128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,\n",
      "           2696,     25,   6790,    220,   2366,     18,    198,  15724,   2696,\n",
      "             25,    220,   1627,   4723,    220,   2366,     19,    271, 128009,\n",
      "         128006,    882, 128007,    271,   9906,     11,   1268,    527,    499,\n",
      "             30, 128009, 128006,  78191, 128007,    271]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0'), 'column_ids': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0'), 'row_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "message = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello, how are you?\"}\n",
    "]\n",
    "\n",
    "message_text = tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=True)\n",
    "inputs = tokenizer(message_text, return_tensors=\"pt\").to(no_grid_model.device)\n",
    "\n",
    "# Prepare the column ids\n",
    "column_ids = torch.ones(inputs.input_ids.shape[0], inputs.input_ids.shape[1], dtype=torch.long).to(no_grid_model.device)\n",
    "row_ids = torch.zeros(inputs.input_ids.shape[0], inputs.input_ids.shape[1], dtype=torch.long).to(no_grid_model.device)\n",
    "inputs[\"column_ids\"] = column_ids\n",
    "inputs[\"row_ids\"] = row_ids\n",
    "\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1 1 1 ... 1 1 1]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [0 0 0 ... 0 0 0]]]\n",
      "[[[42]\n",
      "  [42]\n",
      "  [42]\n",
      "  [42]\n",
      "  [42]\n",
      "  [42]\n",
      "  [42]\n",
      "  [42]\n",
      "  [42]\n",
      "  [42]\n",
      "  [42]\n",
      "  [42]\n",
      "  [42]\n",
      "  [42]\n",
      "  [42]\n",
      "  [42]\n",
      "  [42]\n",
      "  [42]\n",
      "  [42]\n",
      "  [42]\n",
      "  [42]\n",
      "  [42]\n",
      "  [42]\n",
      "  [42]\n",
      "  [42]\n",
      "  [42]\n",
      "  [42]\n",
      "  [42]\n",
      "  [42]\n",
      "  [42]\n",
      "  [42]\n",
      "  [42]]]\n",
      "[[[43]\n",
      "  [43]\n",
      "  [43]\n",
      "  [43]\n",
      "  [43]\n",
      "  [43]\n",
      "  [43]\n",
      "  [43]\n",
      "  [43]\n",
      "  [43]\n",
      "  [43]\n",
      "  [43]\n",
      "  [43]\n",
      "  [43]\n",
      "  [43]\n",
      "  [43]\n",
      "  [43]\n",
      "  [43]\n",
      "  [43]\n",
      "  [43]\n",
      "  [43]\n",
      "  [43]\n",
      "  [43]\n",
      "  [43]\n",
      "  [43]\n",
      "  [43]\n",
      "  [43]\n",
      "  [43]\n",
      "  [43]\n",
      "  [43]\n",
      "  [43]\n",
      "  [43]]]\n",
      "[[[44]\n",
      "  [44]\n",
      "  [44]\n",
      "  [44]\n",
      "  [44]\n",
      "  [44]\n",
      "  [44]\n",
      "  [44]\n",
      "  [44]\n",
      "  [44]\n",
      "  [44]\n",
      "  [44]\n",
      "  [44]\n",
      "  [44]\n",
      "  [44]\n",
      "  [44]\n",
      "  [44]\n",
      "  [44]\n",
      "  [44]\n",
      "  [44]\n",
      "  [44]\n",
      "  [44]\n",
      "  [44]\n",
      "  [44]\n",
      "  [44]\n",
      "  [44]\n",
      "  [44]\n",
      "  [44]\n",
      "  [44]\n",
      "  [44]\n",
      "  [44]\n",
      "  [44]]]\n",
      "[[[45]\n",
      "  [45]\n",
      "  [45]\n",
      "  [45]\n",
      "  [45]\n",
      "  [45]\n",
      "  [45]\n",
      "  [45]\n",
      "  [45]\n",
      "  [45]\n",
      "  [45]\n",
      "  [45]\n",
      "  [45]\n",
      "  [45]\n",
      "  [45]\n",
      "  [45]\n",
      "  [45]\n",
      "  [45]\n",
      "  [45]\n",
      "  [45]\n",
      "  [45]\n",
      "  [45]\n",
      "  [45]\n",
      "  [45]\n",
      "  [45]\n",
      "  [45]\n",
      "  [45]\n",
      "  [45]\n",
      "  [45]\n",
      "  [45]\n",
      "  [45]\n",
      "  [45]]]\n",
      "[[[46]\n",
      "  [46]\n",
      "  [46]\n",
      "  [46]\n",
      "  [46]\n",
      "  [46]\n",
      "  [46]\n",
      "  [46]\n",
      "  [46]\n",
      "  [46]\n",
      "  [46]\n",
      "  [46]\n",
      "  [46]\n",
      "  [46]\n",
      "  [46]\n",
      "  [46]\n",
      "  [46]\n",
      "  [46]\n",
      "  [46]\n",
      "  [46]\n",
      "  [46]\n",
      "  [46]\n",
      "  [46]\n",
      "  [46]\n",
      "  [46]\n",
      "  [46]\n",
      "  [46]\n",
      "  [46]\n",
      "  [46]\n",
      "  [46]\n",
      "  [46]\n",
      "  [46]]]\n",
      "[[[47]\n",
      "  [47]\n",
      "  [47]\n",
      "  [47]\n",
      "  [47]\n",
      "  [47]\n",
      "  [47]\n",
      "  [47]\n",
      "  [47]\n",
      "  [47]\n",
      "  [47]\n",
      "  [47]\n",
      "  [47]\n",
      "  [47]\n",
      "  [47]\n",
      "  [47]\n",
      "  [47]\n",
      "  [47]\n",
      "  [47]\n",
      "  [47]\n",
      "  [47]\n",
      "  [47]\n",
      "  [47]\n",
      "  [47]\n",
      "  [47]\n",
      "  [47]\n",
      "  [47]\n",
      "  [47]\n",
      "  [47]\n",
      "  [47]\n",
      "  [47]\n",
      "  [47]]]\n",
      "[[[48]\n",
      "  [48]\n",
      "  [48]\n",
      "  [48]\n",
      "  [48]\n",
      "  [48]\n",
      "  [48]\n",
      "  [48]\n",
      "  [48]\n",
      "  [48]\n",
      "  [48]\n",
      "  [48]\n",
      "  [48]\n",
      "  [48]\n",
      "  [48]\n",
      "  [48]\n",
      "  [48]\n",
      "  [48]\n",
      "  [48]\n",
      "  [48]\n",
      "  [48]\n",
      "  [48]\n",
      "  [48]\n",
      "  [48]\n",
      "  [48]\n",
      "  [48]\n",
      "  [48]\n",
      "  [48]\n",
      "  [48]\n",
      "  [48]\n",
      "  [48]\n",
      "  [48]]]\n",
      "[[[49]\n",
      "  [49]\n",
      "  [49]\n",
      "  [49]\n",
      "  [49]\n",
      "  [49]\n",
      "  [49]\n",
      "  [49]\n",
      "  [49]\n",
      "  [49]\n",
      "  [49]\n",
      "  [49]\n",
      "  [49]\n",
      "  [49]\n",
      "  [49]\n",
      "  [49]\n",
      "  [49]\n",
      "  [49]\n",
      "  [49]\n",
      "  [49]\n",
      "  [49]\n",
      "  [49]\n",
      "  [49]\n",
      "  [49]\n",
      "  [49]\n",
      "  [49]\n",
      "  [49]\n",
      "  [49]\n",
      "  [49]\n",
      "  [49]\n",
      "  [49]\n",
      "  [49]]]\n",
      "[[[50]\n",
      "  [50]\n",
      "  [50]\n",
      "  [50]\n",
      "  [50]\n",
      "  [50]\n",
      "  [50]\n",
      "  [50]\n",
      "  [50]\n",
      "  [50]\n",
      "  [50]\n",
      "  [50]\n",
      "  [50]\n",
      "  [50]\n",
      "  [50]\n",
      "  [50]\n",
      "  [50]\n",
      "  [50]\n",
      "  [50]\n",
      "  [50]\n",
      "  [50]\n",
      "  [50]\n",
      "  [50]\n",
      "  [50]\n",
      "  [50]\n",
      "  [50]\n",
      "  [50]\n",
      "  [50]\n",
      "  [50]\n",
      "  [50]\n",
      "  [50]\n",
      "  [50]]]\n",
      "system\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Nov 2024\n",
      "\n",
      "user\n",
      "\n",
      "Hello, how are you?assistant\n",
      "\n",
      "I don't know if you are ready to receive\n"
     ]
    }
   ],
   "source": [
    "\n",
    "outputs = no_grid_model.generate(**inputs, max_new_tokens=10)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_no_grid(user_message):\n",
    "    message = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": user_message}\n",
    "    ]\n",
    "    # Tokenize the message\n",
    "    message_text = tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(message_text, return_tensors=\"pt\").to(llama_model.device)\n",
    "    # Generate the response with llama3.2\n",
    "    llama_outputs = llama_model.generate(**inputs, max_new_tokens=64, do_sample=False)\n",
    "    llama_response = tokenizer.decode(llama_outputs[0], skip_special_tokens=True)\n",
    "    no_grid_outputs = no_grid_model.generate(**inputs, max_new_tokens=64, do_sample=False)\n",
    "    no_grid_response = tokenizer.decode(no_grid_outputs[0], skip_special_tokens=True)\n",
    "    return llama_response, no_grid_response\n",
    "    \n",
    "user_messages = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"Why is the sky blue?\",\n",
    "    \"What is the weather in San Francisco?\",\n",
    "]\n",
    "\n",
    "for user_message in user_messages:\n",
    "    llama_response, no_grid_response = test_no_grid(user_message)\n",
    "    print(\"-\" * 100)\n",
    "    print(f\"User message: {user_message}\")\n",
    "    print(f\"Llama response: {llama_response}\")\n",
    "    print(f\"No grid response: {no_grid_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs230",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
