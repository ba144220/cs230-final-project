# Model arguments
model_name: meta-llama/Llama-3.2-1B-Instruct
load_in_4bit: true
load_in_8bit: false

# Table Llama arguments
line_length: 64
x_channels_start: 0
x_channels_end: 32
x_channels_step: 32
# y_channels_start: 0
# y_channels_end: 1
# y_channels_step: 1

# Training arguments
output_dir: /work/cs230-shared/outputs

gradient_accumulation_steps: 4
batch_size: 1
num_train_epochs: 1
save_total_limit: 1
logging_steps: 10
eval_steps: 100
save_steps: 100
max_seq_length: 3072
dry_run: false

run_id_prefix: small

wandb_entity: yuchi-team
wandb_project: cs230-4080-tests

push_to_hub: false
# hf_organization: cs230-table-llama

# Dataset arguments
dataset_root_dir: ./datasets
dataset_names: ["wtq"]
table_extension: csv
train_max_samples_for_each_dataset: 4800
val_max_samples_for_each_dataset: 160
test_max_samples_for_each_dataset: 160
shuffle_seed: 42

max_table_row_num: 40
max_table_width: 64

# Peft arguments
r: 16
lora_alpha: 32
lora_dropout: 0.05
bias: none
task_type: CAUSAL_LM